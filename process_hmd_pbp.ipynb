{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c23414-75b3-4b4f-a69e-e540ed95070c",
   "metadata": {},
   "source": [
    "# Process raw data to HMD - VLIZ workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7bf27-9fe4-40df-bdbb-22e75ca75bb3",
   "metadata": {},
   "source": [
    "This notebook will guide you through all the steps of using [pbp](https://github.com/mbari-org/pbp) to process raw acoustic data into calibrated HMD 1-minute resolution bands. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e1fdf-20d3-49a1-89ba-422c44389dee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275d8de-e602-43a7-bc31-036edc80d7e8",
   "metadata": {},
   "source": [
    "We'll start installing the required dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2d7c8-1842-4d23-bbcb-40c4baad9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b0d45-a323-47e2-9359-33236bdb49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip uninstall mbari-pbp -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a8687-aaef-44c5-84dd-d4dab4610101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install git+https://github.com/lifewatch/pbp.git@rtsys_metadata\n",
    "!{sys.executable} -m pip install cblind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd7998-9a2c-480e-bb0d-568285bec1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package modules\n",
    "import xarray as xr\n",
    "import dask\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import pathlib\n",
    "import yaml\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import pyhydrophone as pyhy\n",
    "import configparser\n",
    "import cblind\n",
    "import matplotlib.pyplot as plt\n",
    "import pypam\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fef403-c2a0-4dab-8eac-0cf4030221bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pbp.meta_gen.gen_soundtrap import SoundTrapMetadataGenerator\n",
    "from pbp.meta_gen.gen_resea import ReseaMetadataGenerator\n",
    "from pbp.logging_helper import create_logger_info, create_logger\n",
    "\n",
    "from pbp.process_helper import ProcessHelper\n",
    "from pbp.file_helper import FileHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a7aaf-3fb5-43fc-98d2-20cba145d79b",
   "metadata": {},
   "source": [
    "We will first indicate where is the json file containing all the metadata for the deployment we want to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fba7db-27ce-4d68-ac9c-1c52a2d9d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location for generated files:\n",
    "path_to_config_file = '/home/jovyan/work/shared/data/AXOLOTL_workshop/metadata/deployments/bpns-Grafton_2022-10-28_EA-SDA14_2003001.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0295d3-29ee-4d09-85bf-17006c63b745",
   "metadata": {},
   "source": [
    "pbp uses yaml files to store metadata on the generated netCDF files - this should be adapted to each institute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599e388-37e6-450f-a3d3-857dd5e37b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attrs_uri = \"/home/jovyan/work/shared/data/AXOLOTL_workshop/metadata/pbp_yaml/globalAttributes.yaml\"\n",
    "variable_attrs_uri = \"/home/jovyan/work/shared/data/AXOLOTL_workshop/metadata/pbp_yaml/variableAttributes.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4829f5-e344-4655-82b5-1541c982e319",
   "metadata": {},
   "source": [
    "We'll load the config file information, and the corresponding instrument config file (for this it's necessary to keep the folder structure the same as in this workshop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b2baf5-c2c2-4461-989f-cd81c230333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = pathlib.Path(path_to_config_file)\n",
    "f = open(path_to_config_file, 'r')\n",
    "deployment_config = json.load(f)\n",
    "\n",
    "instrument_file = config_file.parent.parent.joinpath('receivers', deployment_config['RECORDER_ID'] + '.json')\n",
    "f_i = open(instrument_file, 'r')\n",
    "instrument_config = json.load(f_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128a40b-c4bf-49af-a7a3-b5cf1fc17c98",
   "metadata": {},
   "source": [
    "From the config file, set up the paths to the raw data and the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6575b-f4a1-46a5-ba15-cf24cc299fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A prefix for the name of generate files:\n",
    "deployment_name = deployment_config['DEPLOYMENT_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95330872-b3bd-455d-ab9b-3092c4048473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio data input specifications\n",
    "wav_uri = deployment_config['FOLDER_PATH']\n",
    "# file storage location for the input audio data\n",
    "wav_path = pathlib.Path(urlparse(wav_uri).path)\n",
    "output_dir = config_file.parent.parent.parent.joinpath('HMD', deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d507e8d-2e21-4dd6-bc56-2f08a97f28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_base_dir = wav_path.parent.joinpath('metadata', 'json')  # location to store generated data in JSON format\n",
    "xml_dir = wav_path  # file storage location for the input audio data\n",
    "\n",
    "serial_number = instrument_config['recorder']['serial_number']\n",
    "start_date = pd.to_datetime(deployment_config['AUDIO_START_DEPLOYMENT_DATETIME']).to_pydatetime()  # start date\n",
    "end_date = pd.to_datetime(deployment_config['AUDIO_END_DEPLOYMENT_DATETIME']).to_pydatetime()\n",
    "\n",
    "print(f'We will analyze the period from {start_date} to {end_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8645eb1d-0acf-4a08-a3db-7af4791b0985",
   "metadata": {},
   "source": [
    "We'll read some information from the config file and populate the corresponding fields on the pbp yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c9c8d-265f-4103-bb27-ed82b5ca56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate deployment-specific yaml attributes\n",
    "deployment_attrs_uri = output_dir.joinpath(f'globalAttributes_{deployment_name}.yml')\n",
    "\n",
    "yaml_config = yaml.safe_load(open(global_attrs_uri))\n",
    "yaml_config['creator_name'] = deployment_config['AUTHOR_NAME']\n",
    "yaml_config['creator_email'] = deployment_config['AUTHOR_EMAIL']\n",
    "lon = deployment_config['location']['DEP_LON_DEG']\n",
    "lat = deployment_config['location']['DEP_LAT_DEG']\n",
    "yaml_config['geospatial_bounds'] = f'POINT ({lon} {lat})'\n",
    "yaml_config['comment'] = deployment_config['DEPLOYMENT_COMMENTS']\n",
    "yaml_config['platform'] = deployment_config['location']['MOORING_TYPE']\n",
    "yaml_config['instrument'] = deployment_config['RECORDER_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8be407-e135-40d2-840d-196515d645d1",
   "metadata": {},
   "source": [
    "And we'll create the output directory and copy there the config file used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b3863-006d-4e1d-97db-a42c5932b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not output_dir.exists():\n",
    "    print('Creating a new directory...', output_dir)\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "with open(deployment_attrs_uri, 'w') as file:\n",
    "    yaml.dump(yaml_config, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698ccf3-060a-4f93-8f17-405a61989475",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltage_multiplier = 1.0\n",
    "subset_to = (10, 24000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee2e6-1b19-4381-bfa3-dad460337545",
   "metadata": {},
   "source": [
    "## Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8322c2-6ae7-4ed9-b020-377f0c158fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sens = True \n",
    "# this only applies for RESEA instruments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1480b7f-8223-4c07-84ae-ffd43839aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SOUNDTRAP' in deployment_config['RECORDER_ID']:\n",
    "    hydrophone = pyhy.SoundTrap(model=instrument_config['recorder']['model'].replace('_', ' '),\n",
    "                                serial_number=int(instrument_config['recorder']['serial_number']),\n",
    "                                name=deployment_config['RECORDER_ID'],\n",
    "                                gain_type='High')\n",
    "\n",
    "    print('SoundTrap settings to:')\n",
    "    print('sensitivity: ', hydrophone.sensitivity)\n",
    "    print('Vpp: ', hydrophone.Vpp)\n",
    "    print('preamp_gain: ', hydrophone.preamp_gain)\n",
    "    print('gain_type: ', 'High')\n",
    "\n",
    "    wav_prefix = [f'{serial_number}.']  # prefix for the audio files\n",
    "    seconds_per_file = 300\n",
    "\n",
    "elif 'RESEA' in deployment_config['RECORDER_ID']:\n",
    "    txt_metadata_file = wav_path.parent.joinpath('metadata', 'config.txt')\n",
    "    config_rec = configparser.ConfigParser()\n",
    "    config_rec.read(txt_metadata_file)\n",
    "\n",
    "    byte_per_sample = int(config_rec['Recorder']['format'].split('_')[1]) / 8\n",
    "    sampling_freq = int(config_rec['Recorder']['lp_frequency'].replace('K', '000'))\n",
    "    seconds_per_file = int(config_rec['Recorder']['file_max_size']) * 1E6 / (sampling_freq * byte_per_sample)\n",
    "\n",
    "    hydrophone = pyhy.RTSys(model=instrument_config['recorder']['model'],\n",
    "                            sensitivity=config_rec['Hydrophones']['cha_sh'],\n",
    "                            serial_number=int(instrument_config['recorder']['serial_number'].split('_')[-1]),\n",
    "                            name=deployment_config['RECORDER_ID'],\n",
    "                            Vpp=5,\n",
    "                            preamp_gain=0,\n",
    "                            mode=config_rec['Recorder']['record_mode'])\n",
    "    wav_prefix = ['channelA_', 'channelAC_']  # prefix for the audio files , 'channelA_', 'channelC_'\n",
    "    update_sens = input('Should the RESEA sensitivity be updated with a wav file? '\n",
    "                        'Write no if you do not want any metadata update. '\n",
    "                        'Give the file you wish to use or press enter to use by default the first file.')\n",
    "    if update_sens:\n",
    "        wav_file = list(wav_path.glob('*/*.wav'))[0]\n",
    "        print(f'Using wav file {wav_file} for sensitivity adjustment...')\n",
    "        hydrophone = hydrophone.update_metadata(wav_file)\n",
    "    elif not update_sens:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f'The value {update_sens} is not accepted as an answer to update the RESEA metadata')\n",
    "\n",
    "    print('RESEA settings to:')\n",
    "    print('sensitivity: ', hydrophone.sensitivity)\n",
    "    print('Vpp: ', hydrophone.Vpp)\n",
    "    print('preamp_gain: ', hydrophone.preamp_gain)\n",
    "    print('mode: ', config_rec['Recorder']['record_mode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b97adf-a6eb-40fd-9f4e-c251674963ed",
   "metadata": {},
   "source": [
    "## Start the metadata generation (daily json files pointing to the each minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f3789-8f2e-48ae-b477-748eb871419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = create_logger_info(deployment_config['DEPLOYMENT_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3c97f-2715-4edc-b509-2c99c4b8a6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'SOUNDTRAP' in deployment_config['RECORDER_ID']:\n",
    "    meta_gen = SoundTrapMetadataGenerator(\n",
    "        log=log,\n",
    "        uri=wav_uri,\n",
    "        json_base_dir=str(json_base_dir),\n",
    "        xml_dir=str(xml_dir),\n",
    "        start=start_date.date(),\n",
    "        end=end_date.date(),\n",
    "        prefixes=wav_prefix,\n",
    "        seconds_per_file=seconds_per_file)\n",
    "if 'RESEA' in deployment_config['RECORDER_ID']:\n",
    "    meta_gen = ReseaMetadataGenerator(\n",
    "        log=log,\n",
    "        uri=wav_uri,\n",
    "        json_base_dir=str(json_base_dir),\n",
    "        start=start_date.date(),\n",
    "        end=end_date.date(),\n",
    "        prefixes=wav_prefix,\n",
    "        seconds_per_file=seconds_per_file)\n",
    "\n",
    "\n",
    "meta_gen.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b4af4-64a3-4ad4-bed6-5dd60788da6f",
   "metadata": {},
   "source": [
    "## Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08152cc4-e73f-4d66-a574-8819057c10a8",
   "metadata": {},
   "source": [
    "First we will define a couple of handy functions to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac508e11-aed3-4a84-b57b-584f4a88be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(date: str, gen_netcdf: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to generate the HMB product for a given day.\n",
    "\n",
    "    It makes use of supporting elements in PBP in terms of logging,\n",
    "    file handling, and PyPAM based HMB generation.\n",
    "\n",
    "    :param date: Date to process, in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated\n",
    "    (see parallel execution below).\n",
    "\n",
    "    :return: the generated xarray dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    log_filename = f\"{output_dir}/{deployment_name}{date}.log\"\n",
    "\n",
    "    log = create_logger(\n",
    "        log_filename_and_level=(log_filename, \"INFO\"),\n",
    "        console_level=None,\n",
    "    )\n",
    "\n",
    "    file_helper = FileHelper(\n",
    "        log=log,\n",
    "        json_base_dir=json_base_dir,\n",
    "        gs_client=None,\n",
    "        download_dir=None,\n",
    "    )\n",
    "\n",
    "    process_helper = ProcessHelper(\n",
    "        log=log,\n",
    "        file_helper=file_helper,\n",
    "        output_dir=str(output_dir),\n",
    "        output_prefix=deployment_name + '_',\n",
    "        global_attrs_uri=str(deployment_attrs_uri),\n",
    "        variable_attrs_uri=variable_attrs_uri,\n",
    "        voltage_multiplier=voltage_multiplier,\n",
    "        sensitivity_uri=None,\n",
    "        sensitivity_flat_value=-float(hydrophone.sensitivity),\n",
    "        subset_to=subset_to,\n",
    "    )\n",
    "\n",
    "    # now, get the HMB result:\n",
    "    print(f\"::: Started processing {date=}\")\n",
    "    result = process_helper.process_day(date)\n",
    "\n",
    "    if gen_netcdf:\n",
    "        nc_filename = f\"{output_dir}/{deployment_name}{date}.nc\"\n",
    "        print(f\":::   Ended processing {date=} =>  {nc_filename=}\")\n",
    "    else:\n",
    "        print(f\":::   Ended processing {date=} => (dataset generated in memory)\")\n",
    "\n",
    "    if result is not None:\n",
    "        return result.dataset\n",
    "    else:\n",
    "        print(f\"::: UNEXPECTED: no segments were processed for {date=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7adf1-2ace-4685-8c15-9265d8d89435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_dates(\n",
    "        dates: list[str], gen_netcdf: bool = False\n",
    ") -> list[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Generates HMB for multiple days in parallel using Dask.\n",
    "    Returns the resulting HMB datasets.\n",
    "\n",
    "    :param dates: The dates to process, each in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated.\n",
    "\n",
    "    :return: the list of generated datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    @dask.delayed\n",
    "    def delayed_process_date(date: str):\n",
    "        return process_date(date, gen_netcdf=True)\n",
    "\n",
    "    # To display total elapsed time at the end the processing:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # This will be called by Dask when all dates have completed processing:\n",
    "    def aggregate(*datasets):  # -> list[xr.Dataset]:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\n",
    "            f\"===> All {len(datasets)} dates completed. Elapsed time: {elapsed_time:.1f} seconds ({elapsed_time / 60:.1f} mins)\"\n",
    "        )\n",
    "        return datasets\n",
    "\n",
    "    # Prepare the processes:\n",
    "    delayed_processes = [delayed_process_date(date) for date in dates]\n",
    "    aggregation = dask.delayed(aggregate)(*delayed_processes)\n",
    "\n",
    "    # And launch them:\n",
    "    return aggregation.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4e251-e944-4378-817b-eba401d32593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the dates to process considering start and end date\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='1D')\n",
    "dates = date_range.strftime(\"%Y%m%d\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285ae0d-edb2-43a7-ae3f-8a608fb347a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, launch the generation:\n",
    "print(f\"Launching HMD generation for {len(dates)} {dates=}\")\n",
    "\n",
    "# Get all HMB datasets:\n",
    "generated_datasets = process_multiple_dates(dates, gen_netcdf=True)\n",
    "print(f\"Generated datasets: {len(generated_datasets)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4f67f-dcd7-4c5d-9c28-c0f197142808",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_10min = pypam.utils.join_all_ds_output_deployment(\n",
    "    output_dir,\n",
    "    data_vars=['psd'],\n",
    "    datetime_coord='time',\n",
    "    load=True,\n",
    "    parallel=False,\n",
    "    freq_band=None,\n",
    "    freq_coord='frequency',\n",
    "    engine='netcdf4'\n",
    ")\n",
    "ds = ds_10min.resample(time='1h').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be69a2e-b536-443d-81fe-441db7632fc5",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08006a2-93c5-4a86-ad42-7b1df8adad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afbdfd-6420-4ccc-afff-fa43ece5f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d5f34-4896-41c9-99eb-1d905d5eff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds_10min['psd'].to_pandas()\n",
    "frequency_columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e66f58-745c-4e21-9180-e49d70bdfc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'] = df[frequency_columns].values.tolist()\n",
    "df['month'] = df.index.month \n",
    "df['hour'] = df.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295386c0-ac5f-4861-88f4-9690086b03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "umap_box = umap.UMAP(n_components=2, n_neighbors=20, min_dist=0.05)\n",
    "umap_box.fit(df[frequency_columns].values)\n",
    "embedding = umap_box.transform(df[frequency_columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38cb0b-38a5-4b76-aaa0-3cf0893cf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1],\n",
    "                     s=8, alpha=0.9, hue=df['hour'],\n",
    "                     legend=True)\n",
    "ax.set_xlabel('UMAP x')\n",
    "ax.set_ylabel('UMAP y')\n",
    "ax.set_facecolor('white')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d30c1cb-bcff-416b-ad97-80ba8e0ecdc0",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f33f8-3355-42ae-8b34-dad067f818c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('cb.solstice')\n",
    "ds_10min = pypam.utils.join_all_ds_output_deployment(output_dir,\n",
    "                                               data_vars=['psd'],\n",
    "                                               datetime_coord='time',\n",
    "                                               load=True,\n",
    "                                               parallel=False,\n",
    "                                               freq_band=None,\n",
    "                                               freq_coord='frequency',\n",
    "                                               time_resample='10min'\n",
    "                                           )\n",
    "\n",
    "ds = ds_10min.resample(time='1h').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058455cd-55d6-4375-aa00-ca315f5edf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "cmap = plt.get_cmap('cb.solstice')\n",
    "\n",
    "plots_folder = output_dir.parent.parent.joinpath('plots', config_file.name.replace('.json', ''))\n",
    "if not plots_folder.exists():\n",
    "    os.mkdir(plots_folder)\n",
    "\n",
    "start_plot = pd.to_datetime(datetime.datetime(2022, 10, 30, 13, 0, 0))\n",
    "end_plot = pd.to_datetime(datetime.datetime(2022, 10, 30, 15, 0, 0))\n",
    "ds = ds.where((ds.time > start_plot)  & (ds.time < end_plot), drop=True) \n",
    "# Plot LTSA\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "pypam.plots.plot_2d(ds=ds['psd'], x='time', y='frequency', cmap=cmap, vmin=40, vmax=110,\n",
    "                    cbar_label=r'%s [$%s$]' % (re.sub('_', ' ', ds['psd'].standard_name).title(), ds['psd'].units),\n",
    "                    xlabel='Time', ylabel='Frequency [Hz]', title='Long Term Spectrogram', ylog=True, ax=ax)\n",
    "ax.set_ylim(bottom=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea4741-2f7b-44e7-9c06-9b2c16b3bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SPD\n",
    "percentiles = [1, 10, 50, 90, 99]\n",
    "min_psd = 35  # in db\n",
    "max_psd = 135  # in db\n",
    "h = 1  # in db\n",
    "ds_spd = pypam.utils.compute_spd(ds, data_var='psd', percentiles=percentiles, min_val=min_psd,\n",
    "                                 max_val=max_psd, h=h)\n",
    "\n",
    "pypam.plots.plot_spd(ds_spd, log=True, save_path=plots_folder.joinpath('SPD.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee3e0c-cf68-4f99-a2c6-b82aed383c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the SPL of each band for quality check\n",
    "selected_bands = [[10, 1000], [1000, 10000], [10, float(ds.frequency.max().values)]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "for band in selected_bands:\n",
    "    ds_freq = ds.where((ds.frequency >= band[0]) & (ds.frequency <= band[1]), drop=True)\n",
    "    spl = ds_freq.mean(dim='frequency')\n",
    "    spl.psd.plot(x='time', ax=ax, label=str(band))\n",
    "plt.xlabel('UTC time 10 min aggregation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c94d0-92fb-48c0-806d-d8c19f693670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
